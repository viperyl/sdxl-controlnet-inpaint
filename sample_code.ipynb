{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion XL + Inpaint ckpt + ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, AutoencoderKL, EulerAncestralDiscreteScheduler, UniPCMultistepScheduler\n",
    "from src.pipeline import StableDiffusionXLInpaintControlNetPipeline\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"http_proxy\"] = \"127.0.0.1:15777\"\n",
    "os.environ[\"https_proxy\"] = \"127.0.0.1:15777\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLInpaintControlNetPipeline.from_pretrained(\n",
    "    \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of tiger, hire resolution\"\n",
    "negative_prompt = \"drawing, painting, crayon, sketch, graphite, impressionist, noisy, blurry, soft, deformed, ugly\"\n",
    "\n",
    "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
    "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
    "\n",
    "image = load_image(img_url).resize((1024, 1024))\n",
    "mask = load_image(mask_url).resize((1024, 1024))# Image.fromarray(255 - np.array(load_image(mask_url).resize((1024, 1024))))\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "c_image = np.array(mask)\n",
    "c_image = cv2.Canny(c_image, 100, 200)\n",
    "c_image = c_image[:, :, None]\n",
    "c_image = np.concatenate([c_image, c_image, c_image], axis=2)\n",
    "c_image = Image.fromarray(c_image)\n",
    "\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "images = pipe(\n",
    "                prompt, \n",
    "                # negative_prompt=negative_prompt,\n",
    "                image=image, \n",
    "                mask_image=mask, \n",
    "                control_image=c_image,\n",
    "                num_inference_steps=50,\n",
    "    ).images\n",
    "images[0].save(f\"/home/viper/Result/Week28/ci-final-0.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion XL + Inpaint + ControlNet + Refiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.workflow import sdxl_inpaint_controlnet_refiner\n",
    "from src.utils import cat_image_horizental\n",
    "\n",
    "from diffusers.utils import load_image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"/home/viper/Result/Week25/input.jpg\")\n",
    "mask = load_image(\"/home/viper/Result/Week25/mask.jpg\")\n",
    "\n",
    "c_image = np.array(mask)\n",
    "c_image = cv2.Canny(c_image, 100, 200)\n",
    "c_image = c_image[:, :, None]\n",
    "c_image = np.concatenate([c_image, c_image, c_image], axis=2)\n",
    "c_image = Image.fromarray(c_image)\n",
    "\n",
    "prompt = \"a picture of an earing stand on the marble, high resolution\"\n",
    "negative_prompt = \"low quality, bad quality, blury\"\n",
    "controlnet_conditioning_scale = 1.0  # recommended for good generalization\n",
    "steps = 50\n",
    "guess_mode = False\n",
    "guidance_scale = 7.5\n",
    "strength = 1.0\n",
    "control_guidance_start = 0.0\n",
    "control_guidance_end = 1.0\n",
    "guidance_rescale = 0.0\n",
    "crops_coords_top_left = (0, 0)\n",
    "aesthetic_score = 6.0 # 6.0\n",
    "negative_aesthetic_score = 2.5\n",
    "eta = 0.0\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(114514)\n",
    "\n",
    "pipe = sdxl_inpaint_controlnet_refiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipe(image=image,\n",
    "           mask=mask,\n",
    "           c_image=c_image,\n",
    "           prompt=prompt,\n",
    "           negative_prompt=negative_prompt,\n",
    "           generator=generator,\n",
    "           controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "           guess_mode=guess_mode,\n",
    "           guidance_scale=guidance_scale,\n",
    "           strength=strength,\n",
    "           control_guidance_start=control_guidance_start,\n",
    "           control_guidance_end=control_guidance_end,\n",
    "           guidance_rescale=guidance_rescale,\n",
    "           crops_coords_top_left=crops_coords_top_left,\n",
    "           aesthetic_score=aesthetic_score,\n",
    "           negative_aesthetic_score=negative_aesthetic_score,\n",
    "           eta=eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_image_horizental([image, mask, c_image, res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion XL + Inpaint + ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import ControlNetModel, AutoencoderKL, EulerAncestralDiscreteScheduler\n",
    "from src.pipeline import StableDiffusionXLInpaintControlNetPipeline\n",
    "from diffusers.utils import load_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"diffusers/controlnet-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16)\n",
    "pipe = StableDiffusionXLInpaintControlNetPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"aerial view, a futuristic research complex in a bright foggy jungle, hard lighting\"\n",
    "negative_prompt = 'low quality, bad quality, sketches'\n",
    "\n",
    "image = load_image(\"/home/viper/Result/Week25/input.jpg\")\n",
    "mask = load_image(\"/home/viper/Result/Week25/mask.jpg\")\n",
    "\n",
    "controlnet_conditioning_scale = 0.5  # recommended for good generalization\n",
    "\n",
    "c_image = np.array(mask)\n",
    "c_image = cv2.Canny(c_image, 100, 200)\n",
    "c_image = c_image[:, :, None]\n",
    "c_image = np.concatenate([c_image, c_image, c_image], axis=2)\n",
    "c_image = Image.fromarray(c_image)\n",
    "\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(1145141919)\n",
    "\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "images = pipe(\n",
    "                prompt, \n",
    "                negative_prompt=negative_prompt,\n",
    "                image=image, \n",
    "                mask_image=mask, \n",
    "                control_image=c_image,\n",
    "                num_inference_steps=50,\n",
    "                generator=generator\n",
    "    ).images\n",
    "\n",
    "images[0].save(f\"/home/viper/Result/Week26/ci-final.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
